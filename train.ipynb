{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models2 import EEGformer  # Import the EEGformer model\n",
    "import resampy\n",
    "\n",
    "# Define device and enable Data Parallelism if multiple GPUs are available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using {num_gpus} GPUs\")\n",
    "sampling_rate = 177          # Example sampling rate (samples per second)\n",
    "duration = 3                  # Duration in seconds\n",
    "samples_to_extract = sampling_rate * duration  # Total samples for 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset for loading .npz files\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".npz\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        npz_data = np.load(file_path)\n",
    "\n",
    "        data = resampy.resample(npz_data['data'], sr_orig=npz_data['frequency'], sr_new=sampling_rate)\n",
    "        label = npz_data['label']\n",
    "\n",
    "        num_channels = data.shape[0]\n",
    "        time_steps = data.shape[1]\n",
    "\n",
    "        if samples_to_extract > time_steps:\n",
    "            raise ValueError(f\"Data only has {time_steps} time steps, but {samples_to_extract} are required.\")\n",
    "\n",
    "        data = data[0, :samples_to_extract]  # Select first channel and slice for 3 seconds\n",
    "        data = np.expand_dims(data, axis=0)   # Shape: (1, samples_to_extract)\n",
    "\n",
    "        data_tensor = torch.tensor(data.astype(np.float32))\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return data_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 531, 1])\n"
     ]
    }
   ],
   "source": [
    "# Paths to the processed .npz files\n",
    "train_dir = \"/home/hira/eeg/nmt_events/train\"\n",
    "val_dir = \"/home/hira/eeg/nmt_events/eval\"\n",
    "test_dir = \"/home/hira/eeg/EEG_crops_per_channel/test\"\n",
    "model_saving_path = \"/home/hira/eeg/EEG_crops_per_channel/model/\"\n",
    "model_name = \"eeg_former_v2\"\n",
    "\n",
    "# Initialize the datasets and dataloaders\n",
    "batch_size = 8 * num_gpus  # Adjust batch size according to available GPUs\n",
    "train_dataset = EEGDataset(data_dir=train_dir)\n",
    "val_dataset = EEGDataset(data_dir=val_dir)\n",
    "test_dataset = EEGDataset(data_dir=test_dir)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=10, pin_memory=True)\n",
    "\n",
    "# Parameters\n",
    "input_channels = 1\n",
    "num_cls = 2\n",
    "kernel_size = 10\n",
    "num_blocks = 3\n",
    "num_heads_rtm = 6\n",
    "num_heads_stm = 6\n",
    "num_heads_ttm = 11\n",
    "num_submatrices = 12\n",
    "CF_second = 2\n",
    "\n",
    "# Create a dummy input with shape expected by the model\n",
    "sample_input = torch.randn(8, samples_to_extract, input_channels).to(device)\n",
    "print(sample_input.shape)\n",
    "\n",
    "# Initialize the model\n",
    "model = EEGformer(input=sample_input, num_cls=num_cls, input_channels=input_channels,\n",
    "                  kernel_size=kernel_size, num_blocks=num_blocks, num_heads_RTM=num_heads_rtm,\n",
    "                  num_heads_STM=num_heads_stm, num_heads_TTM=num_heads_ttm,\n",
    "                  num_submatrices=num_submatrices, CF_second=CF_second)\n",
    "\n",
    "\n",
    "\n",
    "# Use Data Parallelism if multiple GPUs are available\n",
    "if num_gpus > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load(model_saving_path + \"eeg_former_v2\" + \".pth\"))\n",
    "\n",
    "# Move the model to GPU(s)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [58/100] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58:   0%|          | 0/5987 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to ODCM: torch.Size([8, 1, 531]) (expected: [batch_size, channels, timesteps])\n",
      "Input shape to ODCM: torch.Size([8, 1, 531]) (expected: [batch_size, channels, timesteps])\n",
      "Input shape to ODCM: torch.Size([8, 1, 531]) (expected: [batch_size, channels, timesteps])\n",
      "Input shape to ODCM: torch.Size([8, 1, 531]) (expected: [batch_size, channels, timesteps])\n",
      "Output shape from ODCM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, ncf, reduced_timesteps])\n",
      "Input shape to RTM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, channels, reduced_timesteps])\n",
      "Output shape from ODCM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, ncf, reduced_timesteps])\n",
      "Input shape to RTM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, channels, reduced_timesteps])\n",
      "Output shape from ODCM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, ncf, reduced_timesteps])\n",
      "Input shape to RTM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, channels, reduced_timesteps])\n",
      "Output shape from ODCM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, ncf, reduced_timesteps])\n",
      "Input shape to RTM: torch.Size([8, 1, 120, 504]) (expected: [batch_size, channels, reduced_timesteps])\n",
      "Output shape from RTM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Input shape to STM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Output shape from RTM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Input shape to STM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Output shape from RTM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Input shape to STM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Output shape from RTM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Input shape to STM: torch.Size([8, 1, 121, 504]) (expected: [batch_size, timesteps, channels, embedding_dim])\n",
      "Output shape from STM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n",
      "Input shape to TTM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n",
      "Output shape from STM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n",
      "Input shape to TTM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n",
      "Output shape from STM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n",
      "Input shape to TTM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n",
      "Output shape from STM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n",
      "Input shape to TTM: torch.Size([8, 121, 2, 504]) (expected: [batch_size, reduced_timesteps, embedding_dim])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58:   0%|          | 0/5987 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), labels\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:118\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 118\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape from TTM: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Input shape to CNN Decoder: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Output shape from TTM: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Input shape to CNN Decoder: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Output shape from CNN Decoder: torch.Size([8, 1, 2]) (expected: [batch_size, num_classes])\n",
      "Output shape from CNN Decoder: torch.Size([8, 1, 2]) (expected: [batch_size, num_classes])\n",
      "Output shape from TTM: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Input shape to CNN Decoder: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Output shape from TTM: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Input shape to CNN Decoder: torch.Size([8, 13, 2, 121]) (expected: [batch_size, submatrices, embedding_dim])\n",
      "Output shape from CNN Decoder: torch.Size([8, 1, 2]) (expected: [batch_size, num_classes])\n",
      "Output shape from CNN Decoder: torch.Size([8, 1, 2]) (expected: [batch_size, num_classes])\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Continue training\n",
    "start_epoch = 57  # Start from the next epoch\n",
    "num_epochs = 100  # Total number of epochs to train (25 more epochs)\n",
    "best_val_acc = 82.72 # Set to the last saved validation accuracy\n",
    "model.train()\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10  # Number of epochs to wait for improvement (3 in this case)\n",
    "counter = 0  # Counter for epochs without improvement\n",
    "\n",
    "for epoch_idx in range(start_epoch, num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    total_train_loss = 0.0\n",
    "    train_true = []\n",
    "    train_preds = []\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch_idx + 1}/{num_epochs}] - Training\")\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Training Epoch {epoch_idx + 1}\") as train_bar:\n",
    "        for batch_idx, (inputs, labels) in train_bar:\n",
    "            if inputs.shape[0] != batch_size:\n",
    "                continue\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Update the progress bar with current loss\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Compute training metrics\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = accuracy_score(train_true, train_preds) * 100\n",
    "    train_precision = precision_score(train_true, train_preds, average='macro', zero_division=0)\n",
    "    train_recall = recall_score(train_true, train_preds, average='macro', zero_division=0)\n",
    "    train_f1 = f1_score(train_true, train_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1-score: {train_f1:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_true = []\n",
    "    val_preds = []\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch_idx + 1}/{num_epochs}] - Validation\")\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc=f\"Validation Epoch {epoch_idx + 1}\") as val_bar:\n",
    "            for batch_idx, (inputs, labels) in val_bar:\n",
    "                if inputs.shape[0] != batch_size:\n",
    "                    continue\n",
    "\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "                val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = accuracy_score(val_true, val_preds) * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # --- Save the model if validation accuracy improved ---\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), model_saving_path + model_name + \".pth\")\n",
    "        print(f\"Model saved at epoch {epoch_idx + 1} with validation accuracy: {val_accuracy:.2f}%\")\n",
    "        counter = 0  # Reset counter when improvement happens\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"Early stopping counter: {counter} out of {patience}\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break #stop the training loop.\n",
    "\n",
    "    model.train()  # Switch back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_model_path = model_saving_path + model_name + '.pth'\n",
    "# print(existing_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import torch\n",
    "\n",
    "# #... your existing code...\n",
    "\n",
    "# # Path to your existing model weights file\n",
    "# existing_model_path = model_saving_path + model_name + \".pth\"\n",
    "\n",
    "# # Path to the new file where you want to copy the weights\n",
    "# new_model_path = model_saving_path + model_name + \"_25epochs_backup.pth\"  # Or any other descriptive name\n",
    "\n",
    "# # Copy the file\n",
    "# shutil.copyfile(existing_model_path, new_model_path)\n",
    "\n",
    "#... continue with your training code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "state_dict = torch.load(model_saving_path + model_name + \".pth\")\n",
    "\n",
    "# If trained with DataParallel, remove \"module.\" prefix\n",
    "if \"module.\" in list(state_dict.keys())[0]:  \n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_state_dict[k.replace(\"module.\", \"\")] = v\n",
    "    state_dict = new_state_dict  # Update state dict\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)  # Move to GPU\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "total_test_loss = 0.0\n",
    "test_true = []\n",
    "test_preds = []\n",
    "\n",
    "print(\"\\nTesting Phase\")\n",
    "with torch.no_grad():\n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=\"Testing\") as test_bar:\n",
    "        for batch_idx, (inputs, labels) in test_bar:\n",
    "            if inputs.shape[0] != batch_size:\n",
    "                continue\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "test_accuracy = accuracy_score(test_true, test_preds) * 100\n",
    "test_precision = precision_score(test_true, test_preds, average='macro', zero_division=0)\n",
    "test_recall = recall_score(test_true, test_preds, average='macro', zero_division=0)\n",
    "test_f1 = f1_score(test_true, test_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
