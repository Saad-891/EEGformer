{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models_duplicate import EEGformer  # Import the EEGformer model\n",
    "import resampy\n",
    "\n",
    "# Define device and enable Data Parallelism if multiple GPUs are available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using {num_gpus} GPUs\")\n",
    "sampling_rate = 177          # Example sampling rate (samples per second)\n",
    "duration = 3                  # Duration in seconds\n",
    "samples_to_extract = sampling_rate * duration  # Total samples for 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset for loading .npz files\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".npz\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        npz_data = np.load(file_path)\n",
    "\n",
    "        data = resampy.resample(npz_data['data'], sr_orig=npz_data['frequency'], sr_new=sampling_rate)\n",
    "        label = npz_data['label']\n",
    "\n",
    "        num_channels = data.shape[0]\n",
    "        time_steps = data.shape[1]\n",
    "\n",
    "        if samples_to_extract > time_steps:\n",
    "            raise ValueError(f\"Data only has {time_steps} time steps, but {samples_to_extract} are required.\")\n",
    "\n",
    "        data = data[0, :samples_to_extract]  # Select first channel and slice for 3 seconds\n",
    "        data = np.expand_dims(data, axis=0)   # Shape: (1, samples_to_extract)\n",
    "\n",
    "        data_tensor = torch.tensor(data.astype(np.float32))\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return data_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 531, 1])\n"
     ]
    }
   ],
   "source": [
    "# Paths to the processed .npz files\n",
    "train_dir = \"/home/hira/eeg/EEG_crops_per_channel/train\"\n",
    "val_dir = \"/home/hira/eeg/EEG_crops_per_channel/eval\"\n",
    "test_dir = \"/home/hira/eeg/EEG_crops_per_channel/test\"\n",
    "model_saving_path = \"/home/hira/eeg/EEG_crops_per_channel/model/\"\n",
    "model_name = \"eeg_former_v1\"\n",
    "\n",
    "# Initialize the datasets and dataloaders\n",
    "batch_size = 8 * num_gpus  # Adjust batch size according to available GPUs\n",
    "train_dataset = EEGDataset(data_dir=train_dir)\n",
    "val_dataset = EEGDataset(data_dir=val_dir)\n",
    "test_dataset = EEGDataset(data_dir=test_dir)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=10, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=10, pin_memory=True)\n",
    "\n",
    "# Parameters\n",
    "input_channels = 1\n",
    "num_cls = 2\n",
    "kernel_size = 10\n",
    "num_blocks = 3\n",
    "num_heads_rtm = 6\n",
    "num_heads_stm = 6\n",
    "num_heads_ttm = 12\n",
    "num_submatrices = 12\n",
    "CF_second = 2\n",
    "\n",
    "# Create a dummy input with shape expected by the model\n",
    "sample_input = torch.randn(8, samples_to_extract, input_channels).to(device)\n",
    "print(sample_input.shape)\n",
    "\n",
    "# Initialize the model\n",
    "model = EEGformer(input=sample_input, num_cls=num_cls, input_channels=input_channels,\n",
    "                  kernel_size=kernel_size, num_blocks=num_blocks, num_heads_RTM=num_heads_rtm,\n",
    "                   num_heads_TTM=num_heads_ttm,\n",
    "                  num_submatrices=num_submatrices, CF_second=CF_second)\n",
    "\n",
    "# Use Data Parallelism if multiple GPUs are available\n",
    "if num_gpus > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Move the model to GPU(s)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 25740/25740 [52:30<00:00,  8.17it/s, loss=0.47]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.5568, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [1/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1: 100%|██████████| 8580/8580 [07:06<00:00, 20.13it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "Model saved at epoch 1 with validation accuracy: 78.03%\n",
      "\n",
      "Epoch [2/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 25740/25740 [29:12<00:00, 14.69it/s, loss=0.626]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [2/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2: 100%|██████████| 8580/8580 [05:53<00:00, 24.30it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [3/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 25740/25740 [27:44<00:00, 15.47it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [3/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 3: 100%|██████████| 8580/8580 [06:12<00:00, 23.02it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [4/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 25740/25740 [27:48<00:00, 15.42it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [4/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 4: 100%|██████████| 8580/8580 [05:34<00:00, 25.69it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [5/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 25740/25740 [27:17<00:00, 15.72it/s, loss=0.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [5/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 5: 100%|██████████| 8580/8580 [06:01<00:00, 23.75it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [6/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 25740/25740 [27:17<00:00, 15.72it/s, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [6/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 6: 100%|██████████| 8580/8580 [05:30<00:00, 25.98it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [7/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 25740/25740 [27:55<00:00, 15.36it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [7/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 7: 100%|██████████| 8580/8580 [05:50<00:00, 24.48it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [8/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 25740/25740 [27:25<00:00, 15.64it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [8/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 8: 100%|██████████| 8580/8580 [05:53<00:00, 24.26it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [9/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 25740/25740 [27:03<00:00, 15.86it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [9/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 9: 100%|██████████| 8580/8580 [05:41<00:00, 25.12it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n",
      "\n",
      "Epoch [10/10] - Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 25740/25740 [43:05<00:00,  9.96it/s, loss=0.501]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5330, Accuracy: 78.03%\n",
      "Precision: 0.3901, Recall: 0.5000, F1-score: 0.4383\n",
      "\n",
      "Epoch [10/10] - Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 10: 100%|██████████| 8580/8580 [10:50<00:00, 13.18it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5329, Accuracy: 78.03%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    total_train_loss = 0.0\n",
    "    train_true = []\n",
    "    train_preds = []\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch_idx + 1}/{num_epochs}] - Training\")\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Training Epoch {epoch_idx + 1}\") as train_bar:\n",
    "        for batch_idx, (inputs, labels) in train_bar:\n",
    "            if inputs.shape[0] != batch_size:\n",
    "                continue\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Update the progress bar with current loss\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Compute training metrics\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = accuracy_score(train_true, train_preds) * 100\n",
    "    train_precision = precision_score(train_true, train_preds, average='macro', zero_division=0)\n",
    "    train_recall = recall_score(train_true, train_preds, average='macro', zero_division=0)\n",
    "    train_f1 = f1_score(train_true, train_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1-score: {train_f1:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_true = []\n",
    "    val_preds = []\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch_idx + 1}/{num_epochs}] - Validation\")\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc=f\"Validation Epoch {epoch_idx + 1}\") as val_bar:\n",
    "            for batch_idx, (inputs, labels) in val_bar:\n",
    "                if inputs.shape[0] != batch_size:\n",
    "                    continue\n",
    "\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "                val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = accuracy_score(val_true, val_preds) * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # --- Save the model if validation accuracy improved ---\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), model_saving_path + model_name + \".pth\")\n",
    "        print(f\"Model saved at epoch {epoch_idx + 1} with validation accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    model.train()  # Switch back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.projection_layer.weight\", \"module.projection_layer.bias\", \"module.odcm.cvf1.weight\", \"module.odcm.cvf1.bias\", \"module.odcm.cvf2.weight\", \"module.odcm.cvf2.bias\", \"module.odcm.cvf3.weight\", \"module.odcm.cvf3.bias\", \"module.rtm.weight\", \"module.rtm.bias\", \"module.rtm.cls\", \"module.rtm.tfb.0.Wqkv\", \"module.rtm.tfb.0.Wo\", \"module.rtm.tfb.0.lnorm.weight\", \"module.rtm.tfb.0.lnorm.bias\", \"module.rtm.tfb.0.lnormz.weight\", \"module.rtm.tfb.0.lnormz.bias\", \"module.rtm.tfb.0.mlp.fc1.weight\", \"module.rtm.tfb.0.mlp.fc1.bias\", \"module.rtm.tfb.0.mlp.fc2.weight\", \"module.rtm.tfb.0.mlp.fc2.bias\", \"module.rtm.tfb.1.Wqkv\", \"module.rtm.tfb.1.Wo\", \"module.rtm.tfb.1.lnorm.weight\", \"module.rtm.tfb.1.lnorm.bias\", \"module.rtm.tfb.1.lnormz.weight\", \"module.rtm.tfb.1.lnormz.bias\", \"module.rtm.tfb.1.mlp.fc1.weight\", \"module.rtm.tfb.1.mlp.fc1.bias\", \"module.rtm.tfb.1.mlp.fc2.weight\", \"module.rtm.tfb.1.mlp.fc2.bias\", \"module.rtm.tfb.2.Wqkv\", \"module.rtm.tfb.2.Wo\", \"module.rtm.tfb.2.lnorm.weight\", \"module.rtm.tfb.2.lnorm.bias\", \"module.rtm.tfb.2.lnormz.weight\", \"module.rtm.tfb.2.lnormz.bias\", \"module.rtm.tfb.2.mlp.fc1.weight\", \"module.rtm.tfb.2.mlp.fc1.bias\", \"module.rtm.tfb.2.mlp.fc2.weight\", \"module.rtm.tfb.2.mlp.fc2.bias\", \"module.ttm.weight\", \"module.ttm.bias\", \"module.ttm.cls\", \"module.ttm.projection_layer.weight\", \"module.ttm.projection_layer.bias\", \"module.ttm.tfb.0.Wqkv\", \"module.ttm.tfb.0.Wo\", \"module.ttm.tfb.0.lnorm.weight\", \"module.ttm.tfb.0.lnorm.bias\", \"module.ttm.tfb.0.lnormz.weight\", \"module.ttm.tfb.0.lnormz.bias\", \"module.ttm.tfb.0.mlp.fc1.weight\", \"module.ttm.tfb.0.mlp.fc1.bias\", \"module.ttm.tfb.0.mlp.fc2.weight\", \"module.ttm.tfb.0.mlp.fc2.bias\", \"module.ttm.tfb.1.Wqkv\", \"module.ttm.tfb.1.Wo\", \"module.ttm.tfb.1.lnorm.weight\", \"module.ttm.tfb.1.lnorm.bias\", \"module.ttm.tfb.1.lnormz.weight\", \"module.ttm.tfb.1.lnormz.bias\", \"module.ttm.tfb.1.mlp.fc1.weight\", \"module.ttm.tfb.1.mlp.fc1.bias\", \"module.ttm.tfb.1.mlp.fc2.weight\", \"module.ttm.tfb.1.mlp.fc2.bias\", \"module.ttm.tfb.2.Wqkv\", \"module.ttm.tfb.2.Wo\", \"module.ttm.tfb.2.lnorm.weight\", \"module.ttm.tfb.2.lnorm.bias\", \"module.ttm.tfb.2.lnormz.weight\", \"module.ttm.tfb.2.lnormz.bias\", \"module.ttm.tfb.2.mlp.fc1.weight\", \"module.ttm.tfb.2.mlp.fc1.bias\", \"module.ttm.tfb.2.mlp.fc2.weight\", \"module.ttm.tfb.2.mlp.fc2.bias\", \"module.ttm.lnorm_extra.weight\", \"module.ttm.lnorm_extra.bias\", \"module.cnndecoder.cvd1.weight\", \"module.cnndecoder.cvd1.bias\", \"module.cnndecoder.cvd2.weight\", \"module.cnndecoder.cvd2.bias\", \"module.cnndecoder.cvd3.weight\", \"module.cnndecoder.cvd3.bias\", \"module.cnndecoder.fc.weight\", \"module.cnndecoder.fc.bias\". \n\tUnexpected key(s) in state_dict: \"projection_layer.weight\", \"projection_layer.bias\", \"odcm.cvf1.weight\", \"odcm.cvf1.bias\", \"odcm.cvf2.weight\", \"odcm.cvf2.bias\", \"odcm.cvf3.weight\", \"odcm.cvf3.bias\", \"rtm.weight\", \"rtm.bias\", \"rtm.cls\", \"rtm.tfb.0.Wqkv\", \"rtm.tfb.0.Wo\", \"rtm.tfb.0.lnorm.weight\", \"rtm.tfb.0.lnorm.bias\", \"rtm.tfb.0.lnormz.weight\", \"rtm.tfb.0.lnormz.bias\", \"rtm.tfb.0.mlp.fc1.weight\", \"rtm.tfb.0.mlp.fc1.bias\", \"rtm.tfb.0.mlp.fc2.weight\", \"rtm.tfb.0.mlp.fc2.bias\", \"rtm.tfb.1.Wqkv\", \"rtm.tfb.1.Wo\", \"rtm.tfb.1.lnorm.weight\", \"rtm.tfb.1.lnorm.bias\", \"rtm.tfb.1.lnormz.weight\", \"rtm.tfb.1.lnormz.bias\", \"rtm.tfb.1.mlp.fc1.weight\", \"rtm.tfb.1.mlp.fc1.bias\", \"rtm.tfb.1.mlp.fc2.weight\", \"rtm.tfb.1.mlp.fc2.bias\", \"rtm.tfb.2.Wqkv\", \"rtm.tfb.2.Wo\", \"rtm.tfb.2.lnorm.weight\", \"rtm.tfb.2.lnorm.bias\", \"rtm.tfb.2.lnormz.weight\", \"rtm.tfb.2.lnormz.bias\", \"rtm.tfb.2.mlp.fc1.weight\", \"rtm.tfb.2.mlp.fc1.bias\", \"rtm.tfb.2.mlp.fc2.weight\", \"rtm.tfb.2.mlp.fc2.bias\", \"ttm.weight\", \"ttm.bias\", \"ttm.cls\", \"ttm.projection_layer.weight\", \"ttm.projection_layer.bias\", \"ttm.tfb.0.Wqkv\", \"ttm.tfb.0.Wo\", \"ttm.tfb.0.lnorm.weight\", \"ttm.tfb.0.lnorm.bias\", \"ttm.tfb.0.lnormz.weight\", \"ttm.tfb.0.lnormz.bias\", \"ttm.tfb.0.mlp.fc1.weight\", \"ttm.tfb.0.mlp.fc1.bias\", \"ttm.tfb.0.mlp.fc2.weight\", \"ttm.tfb.0.mlp.fc2.bias\", \"ttm.tfb.1.Wqkv\", \"ttm.tfb.1.Wo\", \"ttm.tfb.1.lnorm.weight\", \"ttm.tfb.1.lnorm.bias\", \"ttm.tfb.1.lnormz.weight\", \"ttm.tfb.1.lnormz.bias\", \"ttm.tfb.1.mlp.fc1.weight\", \"ttm.tfb.1.mlp.fc1.bias\", \"ttm.tfb.1.mlp.fc2.weight\", \"ttm.tfb.1.mlp.fc2.bias\", \"ttm.tfb.2.Wqkv\", \"ttm.tfb.2.Wo\", \"ttm.tfb.2.lnorm.weight\", \"ttm.tfb.2.lnorm.bias\", \"ttm.tfb.2.lnormz.weight\", \"ttm.tfb.2.lnormz.bias\", \"ttm.tfb.2.mlp.fc1.weight\", \"ttm.tfb.2.mlp.fc1.bias\", \"ttm.tfb.2.mlp.fc2.weight\", \"ttm.tfb.2.mlp.fc2.bias\", \"ttm.lnorm_extra.weight\", \"ttm.lnorm_extra.bias\", \"cnndecoder.cvd1.weight\", \"cnndecoder.cvd1.bias\", \"cnndecoder.cvd2.weight\", \"cnndecoder.cvd2.bias\", \"cnndecoder.cvd3.weight\", \"cnndecoder.cvd3.bias\", \"cnndecoder.fc.weight\", \"cnndecoder.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hira/eeg/code/train_duplicate.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.19.10.196/home/hira/eeg/code/train_duplicate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         new_state_dict[k\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mmodule.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)] \u001b[39m=\u001b[39m v\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.19.10.196/home/hira/eeg/code/train_duplicate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     state_dict \u001b[39m=\u001b[39m new_state_dict  \u001b[39m# Update state dict\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.19.10.196/home/hira/eeg/code/train_duplicate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(state_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.19.10.196/home/hira/eeg/code/train_duplicate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)  \u001b[39m# Move to GPU\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.19.10.196/home/hira/eeg/code/train_duplicate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39meval()  \u001b[39m# Set model to evaluation mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[39m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.projection_layer.weight\", \"module.projection_layer.bias\", \"module.odcm.cvf1.weight\", \"module.odcm.cvf1.bias\", \"module.odcm.cvf2.weight\", \"module.odcm.cvf2.bias\", \"module.odcm.cvf3.weight\", \"module.odcm.cvf3.bias\", \"module.rtm.weight\", \"module.rtm.bias\", \"module.rtm.cls\", \"module.rtm.tfb.0.Wqkv\", \"module.rtm.tfb.0.Wo\", \"module.rtm.tfb.0.lnorm.weight\", \"module.rtm.tfb.0.lnorm.bias\", \"module.rtm.tfb.0.lnormz.weight\", \"module.rtm.tfb.0.lnormz.bias\", \"module.rtm.tfb.0.mlp.fc1.weight\", \"module.rtm.tfb.0.mlp.fc1.bias\", \"module.rtm.tfb.0.mlp.fc2.weight\", \"module.rtm.tfb.0.mlp.fc2.bias\", \"module.rtm.tfb.1.Wqkv\", \"module.rtm.tfb.1.Wo\", \"module.rtm.tfb.1.lnorm.weight\", \"module.rtm.tfb.1.lnorm.bias\", \"module.rtm.tfb.1.lnormz.weight\", \"module.rtm.tfb.1.lnormz.bias\", \"module.rtm.tfb.1.mlp.fc1.weight\", \"module.rtm.tfb.1.mlp.fc1.bias\", \"module.rtm.tfb.1.mlp.fc2.weight\", \"module.rtm.tfb.1.mlp.fc2.bias\", \"module.rtm.tfb.2.Wqkv\", \"module.rtm.tfb.2.Wo\", \"module.rtm.tfb.2.lnorm.weight\", \"module.rtm.tfb.2.lnorm.bias\", \"module.rtm.tfb.2.lnormz.weight\", \"module.rtm.tfb.2.lnormz.bias\", \"module.rtm.tfb.2.mlp.fc1.weight\", \"module.rtm.tfb.2.mlp.fc1.bias\", \"module.rtm.tfb.2.mlp.fc2.weight\", \"module.rtm.tfb.2.mlp.fc2.bias\", \"module.ttm.weight\", \"module.ttm.bias\", \"module.ttm.cls\", \"module.ttm.projection_layer.weight\", \"module.ttm.projection_layer.bias\", \"module.ttm.tfb.0.Wqkv\", \"module.ttm.tfb.0.Wo\", \"module.ttm.tfb.0.lnorm.weight\", \"module.ttm.tfb.0.lnorm.bias\", \"module.ttm.tfb.0.lnormz.weight\", \"module.ttm.tfb.0.lnormz.bias\", \"module.ttm.tfb.0.mlp.fc1.weight\", \"module.ttm.tfb.0.mlp.fc1.bias\", \"module.ttm.tfb.0.mlp.fc2.weight\", \"module.ttm.tfb.0.mlp.fc2.bias\", \"module.ttm.tfb.1.Wqkv\", \"module.ttm.tfb.1.Wo\", \"module.ttm.tfb.1.lnorm.weight\", \"module.ttm.tfb.1.lnorm.bias\", \"module.ttm.tfb.1.lnormz.weight\", \"module.ttm.tfb.1.lnormz.bias\", \"module.ttm.tfb.1.mlp.fc1.weight\", \"module.ttm.tfb.1.mlp.fc1.bias\", \"module.ttm.tfb.1.mlp.fc2.weight\", \"module.ttm.tfb.1.mlp.fc2.bias\", \"module.ttm.tfb.2.Wqkv\", \"module.ttm.tfb.2.Wo\", \"module.ttm.tfb.2.lnorm.weight\", \"module.ttm.tfb.2.lnorm.bias\", \"module.ttm.tfb.2.lnormz.weight\", \"module.ttm.tfb.2.lnormz.bias\", \"module.ttm.tfb.2.mlp.fc1.weight\", \"module.ttm.tfb.2.mlp.fc1.bias\", \"module.ttm.tfb.2.mlp.fc2.weight\", \"module.ttm.tfb.2.mlp.fc2.bias\", \"module.ttm.lnorm_extra.weight\", \"module.ttm.lnorm_extra.bias\", \"module.cnndecoder.cvd1.weight\", \"module.cnndecoder.cvd1.bias\", \"module.cnndecoder.cvd2.weight\", \"module.cnndecoder.cvd2.bias\", \"module.cnndecoder.cvd3.weight\", \"module.cnndecoder.cvd3.bias\", \"module.cnndecoder.fc.weight\", \"module.cnndecoder.fc.bias\". \n\tUnexpected key(s) in state_dict: \"projection_layer.weight\", \"projection_layer.bias\", \"odcm.cvf1.weight\", \"odcm.cvf1.bias\", \"odcm.cvf2.weight\", \"odcm.cvf2.bias\", \"odcm.cvf3.weight\", \"odcm.cvf3.bias\", \"rtm.weight\", \"rtm.bias\", \"rtm.cls\", \"rtm.tfb.0.Wqkv\", \"rtm.tfb.0.Wo\", \"rtm.tfb.0.lnorm.weight\", \"rtm.tfb.0.lnorm.bias\", \"rtm.tfb.0.lnormz.weight\", \"rtm.tfb.0.lnormz.bias\", \"rtm.tfb.0.mlp.fc1.weight\", \"rtm.tfb.0.mlp.fc1.bias\", \"rtm.tfb.0.mlp.fc2.weight\", \"rtm.tfb.0.mlp.fc2.bias\", \"rtm.tfb.1.Wqkv\", \"rtm.tfb.1.Wo\", \"rtm.tfb.1.lnorm.weight\", \"rtm.tfb.1.lnorm.bias\", \"rtm.tfb.1.lnormz.weight\", \"rtm.tfb.1.lnormz.bias\", \"rtm.tfb.1.mlp.fc1.weight\", \"rtm.tfb.1.mlp.fc1.bias\", \"rtm.tfb.1.mlp.fc2.weight\", \"rtm.tfb.1.mlp.fc2.bias\", \"rtm.tfb.2.Wqkv\", \"rtm.tfb.2.Wo\", \"rtm.tfb.2.lnorm.weight\", \"rtm.tfb.2.lnorm.bias\", \"rtm.tfb.2.lnormz.weight\", \"rtm.tfb.2.lnormz.bias\", \"rtm.tfb.2.mlp.fc1.weight\", \"rtm.tfb.2.mlp.fc1.bias\", \"rtm.tfb.2.mlp.fc2.weight\", \"rtm.tfb.2.mlp.fc2.bias\", \"ttm.weight\", \"ttm.bias\", \"ttm.cls\", \"ttm.projection_layer.weight\", \"ttm.projection_layer.bias\", \"ttm.tfb.0.Wqkv\", \"ttm.tfb.0.Wo\", \"ttm.tfb.0.lnorm.weight\", \"ttm.tfb.0.lnorm.bias\", \"ttm.tfb.0.lnormz.weight\", \"ttm.tfb.0.lnormz.bias\", \"ttm.tfb.0.mlp.fc1.weight\", \"ttm.tfb.0.mlp.fc1.bias\", \"ttm.tfb.0.mlp.fc2.weight\", \"ttm.tfb.0.mlp.fc2.bias\", \"ttm.tfb.1.Wqkv\", \"ttm.tfb.1.Wo\", \"ttm.tfb.1.lnorm.weight\", \"ttm.tfb.1.lnorm.bias\", \"ttm.tfb.1.lnormz.weight\", \"ttm.tfb.1.lnormz.bias\", \"ttm.tfb.1.mlp.fc1.weight\", \"ttm.tfb.1.mlp.fc1.bias\", \"ttm.tfb.1.mlp.fc2.weight\", \"ttm.tfb.1.mlp.fc2.bias\", \"ttm.tfb.2.Wqkv\", \"ttm.tfb.2.Wo\", \"ttm.tfb.2.lnorm.weight\", \"ttm.tfb.2.lnorm.bias\", \"ttm.tfb.2.lnormz.weight\", \"ttm.tfb.2.lnormz.bias\", \"ttm.tfb.2.mlp.fc1.weight\", \"ttm.tfb.2.mlp.fc1.bias\", \"ttm.tfb.2.mlp.fc2.weight\", \"ttm.tfb.2.mlp.fc2.bias\", \"ttm.lnorm_extra.weight\", \"ttm.lnorm_extra.bias\", \"cnndecoder.cvd1.weight\", \"cnndecoder.cvd1.bias\", \"cnndecoder.cvd2.weight\", \"cnndecoder.cvd2.bias\", \"cnndecoder.cvd3.weight\", \"cnndecoder.cvd3.bias\", \"cnndecoder.fc.weight\", \"cnndecoder.fc.bias\". "
     ]
    }
   ],
   "source": [
    "# Load the best model weights\n",
    "state_dict = torch.load(model_saving_path + model_name + \".pth\")\n",
    "\n",
    "# If trained with DataParallel, remove \"module.\" prefix\n",
    "if \"module.\" in list(state_dict.keys())[0]:  \n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_state_dict[k.replace(\"module.\", \"\")] = v\n",
    "    state_dict = new_state_dict  # Update state dict\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)  # Move to GPU\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "total_test_loss = 0.0\n",
    "test_true = []\n",
    "test_preds = []\n",
    "\n",
    "print(\"\\nTesting Phase\")\n",
    "with torch.no_grad():\n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=\"Testing\") as test_bar:\n",
    "        for batch_idx, (inputs, labels) in test_bar:\n",
    "            if inputs.shape[0] != batch_size:\n",
    "                continue\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            test_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "test_accuracy = accuracy_score(test_true, test_preds) * 100\n",
    "test_precision = precision_score(test_true, test_preds, average='macro', zero_division=0)\n",
    "test_recall = recall_score(test_true, test_preds, average='macro', zero_division=0)\n",
    "test_f1 = f1_score(test_true, test_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
